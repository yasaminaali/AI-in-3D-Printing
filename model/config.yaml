# CNN+RNN Neural Network Configuration
# Paper-compliant hybrid architecture for Hamiltonian path optimization

# Model Architecture (as per CCAI paper Section 3.3)
model:
  name: "CNN_RNN_Hamiltonian"
  
  # CNN Configuration
  cnn:
    input_channels: 4  # H_edges, V_edges, Nodes, Zone_boundary
    embedding_dim: 128  # Paper: 128-dimensional CNN embedding
    num_layers: 4  # Paper: 4 convolutional layers
    kernel_size: 3  # Paper: 3x3 convolutions
    padding: 1  # Paper: padding to maintain spatial dimensions
    channel_progression: [16, 32, 64, 128]  # Increasing complexity
    
  # RNN Configuration  
  rnn:
    type: "GRU"  # Paper: Gated Recurrent Unit
    hidden_size: 256
    num_layers: 2
    dropout: 0.3  # Increased dropout to prevent overfitting
    bidirectional: false  # Sequential operations are directional
    
  # Operation prediction head
  predictor:
    sequence_length: 200  # Increased to capture full optimization trajectories (was 50)
    operation_types: ["T", "F", "N"]  # Transpose, Flip, No-op (N = stop token)
    max_positions: 30  # Focus on 30x30 grids (most common in dataset)
    flip_variants: ["n", "s", "e", "w"]  # north, south, east, west
    transpose_variants: ["nl", "nr", "sl", "sr", "eb"]  # corner-based variants

# Training Configuration (Optimized for RTX 5090 32GB)
training:
  epochs: 100              # Full training with early stopping
  learning_rate: 0.001     # AdamW learning rate
  min_learning_rate: 0.00001  # Minimum LR for cosine annealing
  warmup_epochs: 2         # Warmup epochs
  optimizer: "AdamW"       # AdamW for better weight decay handling
  weight_decay: 0.01       # Regularization
  scheduler: "CosineAnnealing"
  dropout: 0.3             # Dropout rate
  gradient_clip: 1.0
  
  # Batch size reduced for longer sequences (200 vs 50)
  batch_size: 16  # Reduced from 64 to accommodate 4x longer sequences
  
  # Full sequence training (ALL operations, not just effective)
  effective_only: false    # Train on ALL ops (setup + reducing)
  max_trajectories: null   # Process ALL trajectories (null = no limit)
  cache_file: "full_trajectories.pkl"  # Cache preprocessed samples
  
  # Loss weights
  loss_weights:
    operation_type: 1.0
    position_x: 1.0
    position_y: 1.0
    variant: 1.0
  label_smoothing: 0.1
  early_stopping_patience: 20  # Stop if no improvement for 20 epochs

# Data Configuration
data:
  dataset_path: "/root/AI-in-3D-Printing/combined_dataset.jsonl"
  output_dir: "nn_data"
  
  # Grid size
  grid_size: 30
  
  # Filtering
  filter:
    min_improvement: 1  # Discard records with no improvement
    max_sequence_length: 500
    min_crossing_reduction_ratio: 0.4  # At least 40% improvement
    
  # Stratified split (80/10/10 per grid size)
  split:
    train_ratio: 0.8
    val_ratio: 0.1
    test_ratio: 0.1
    sort_by: "final_crossings"  # Sort by performance for stratification
    ascending: true  # Best performers first (for training priority)
    
  # Augmentation (enabled for robustness and regularization)
  augmentation:
    enabled: true
    dropout_operations: 0.1  # Randomly drop 10% of operations in sequence
    noise_positions: 0.05    # Add small noise to encourage generalization
    shuffle_equivalent: false  # Don't shuffle - order matters
    zone_permutation: false

# Validation & Testing Configuration
validation:
  # How often to run validation (1 = every epoch, 5 = every 5th epoch)
  validate_every_n_epochs: 1
  
  # Performance statistics computation (slow, toggle off for speed)
  compute_performance_stats: false  # DISABLED for speed
  stats_sample_size: 20  # Number of samples for fast stats computation
  detailed_stats_every_n_epochs: 25  # Full detailed stats every N epochs
  
  # Metrics to track
  metrics:
    - "crossing_reduction"
    - "sequence_validity"
    - "path_hamiltonicity"
    - "mean_absolute_error"
    - "top_k_accuracy"

# Checkpointing Configuration
checkpointing:
  checkpoint_dir: "nn_checkpoints"
  save_best_only: false  # Save best + periodic
  save_every_n_epochs: 20  # Save every 20 epochs (5 checkpoints in 100 epochs)
  keep_last_n_checkpoints: 5  # Keep last 5 periodic checkpoints

# Logging Configuration
logging:
  log_dir: "nn_logs"
  log_to_csv: true  # Save training metrics to CSV
  log_to_tensorboard: false  # Requires tensorboard package
  log_interval: 10  # Log every N batches
  console_output: true  # Print to console
  
# Hardware Configuration
hardware:
  device: "cuda"  # GPU - RTX 5090 with PyTorch 2.7.0 nightly
  num_workers: 8  # DataLoader workers (0 = main process, faster for small datasets)
  pin_memory: true
  persistent_workers: true  # Keep workers alive between epochs for speed  
# Performance Tuning
performance:
  # Gradient accumulation (effective batch size = batch_size * accumulation_steps)
  # Use this to simulate larger batches without increasing memory
  gradient_accumulation_steps: 1
  
  # Mixed precision training - ENABLED for RTX 5090
  mixed_precision: true

  # Compile model (PyTorch 2.0+ feature)
  compile_model: false
  
# Inference Configuration
inference:
  max_operations: 100
  beam_width: 5  # Beam search width for operation sequences
  temperature: 1.0  # Sampling temperature
  top_k: 10  # Top-k sampling for operations
