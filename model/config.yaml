# CNN+RNN Neural Network Configuration
# Paper-compliant hybrid architecture for Hamiltonian path optimization

# Model Architecture (as per CCAI paper Section 3.3)
model:
  name: "CNN_RNN_Hamiltonian"
  
  # CNN Configuration
  cnn:
    input_channels: 4  # H_edges, V_edges, Nodes, Zone_boundary
    embedding_dim: 128  # Paper: 128-dimensional CNN embedding
    num_layers: 4  # Paper: 4 convolutional layers
    kernel_size: 3  # Paper: 3x3 convolutions
    padding: 1  # Paper: padding to maintain spatial dimensions
    channel_progression: [16, 32, 64, 128]  # Increasing complexity
    
  # RNN Configuration  
  rnn:
    type: "GRU"  # Paper: Gated Recurrent Unit
    hidden_size: 256
    num_layers: 2
    dropout: 0.2
    bidirectional: false  # Sequential operations are directional
    
  # Operation prediction head
  predictor:
    sequence_length: 100  # Truncated to practical length (data has outliers at 490)
    operation_types: ["T", "F", "N"]  # Transpose, Flip, No-op
    max_positions: 30  # Focus on 30x30 grids (most common in dataset)
    flip_variants: ["n", "s", "e", "w"]  # north, south, east, west
    transpose_variants: ["nl", "nr", "sl", "sr", "eb"]  # corner-based variants

# Training Configuration (Optimized for RTX 4080 12GB)
training:
  epochs: 100
  learning_rate: 0.001
  optimizer: "Adam"
  weight_decay: 0.0001
  scheduler: "ReduceLROnPlateau"
  scheduler_patience: 10
  early_stopping_patience: 20
  gradient_clip: 1.0
  
  # Batch sizes per grid size (configurable per hardware)
  # Format: "WxH": batch_size
  # Adjust based on your GPU memory
  batch_sizes:
    "30x30": 64   # For 30x30 grids
    "50x50": 32   # For 50x50 grids
    "80x80": 16   # For 80x80 grids
    "default": 8  # Fallback for other sizes
  
  # Loss weights
  loss_weights:
    operation_type: 1.0
    position_x: 1.0
    position_y: 1.0
    variant: 0.5
    crossing_reduction: 2.0  # Emphasize crossing minimization
    sequence_validity: 1.0  # Ensure valid Hamiltonian paths

# Data Configuration
data:
  dataset_path: "output/datasets/combined_dataset.jsonl"
  output_dir: "nn_data"
  
  # Grid size categories for stratified split
  grid_sizes: [10, 15, 20, 25, 30]
  
  # Filtering
  filter:
    min_improvement: 1  # Discard records with no improvement
    max_sequence_length: 500
    min_crossing_reduction_ratio: 0.1  # At least 10% improvement
    
  # Stratified split (80/10/10 per grid size)
  split:
    train_ratio: 0.8
    val_ratio: 0.1
    test_ratio: 0.1
    sort_by: "final_crossings"  # Sort by performance for stratification
    ascending: true  # Best performers first (for training priority)
    
  # Augmentation (optional, for robustness)
  augmentation:
    enabled: false
    rotation: false  # Hamiltonian paths have directional constraints
    flip_grid: false
    zone_permutation: false

# Validation & Testing Configuration
validation:
  # How often to run validation (1 = every epoch, 5 = every 5th epoch)
  validate_every_n_epochs: 1
  
  # Performance statistics computation (slow, toggle off for speed)
  compute_performance_stats: false  # DISABLED - set to true only when needed
  stats_sample_size: 10  # Number of samples for fast stats computation
  detailed_stats_every_n_epochs: 5  # Full detailed stats every N epochs (if compute_performance_stats is true)
  
  # Metrics to track
  metrics:
    - "crossing_reduction"
    - "sequence_validity"
    - "path_hamiltonicity"
    - "mean_absolute_error"
    - "top_k_accuracy"

# Checkpointing Configuration
checkpointing:
  checkpoint_dir: "nn_checkpoints"
  save_best_only: true  # Only save when validation improves
  save_every_n_epochs: 10  # Also save periodic checkpoint every N epochs
  keep_last_n_checkpoints: 3  # Keep only last N periodic checkpoints (0 = keep all)

# Logging Configuration
logging:
  log_dir: "nn_logs"
  log_to_csv: true  # Save training metrics to CSV
  log_to_tensorboard: false  # Requires tensorboard package
  log_interval: 10  # Log every N batches
  console_output: true  # Print to console
  
# Hardware Configuration (STRICTLY GPU)
hardware:
  device: "cuda"  # Force GPU usage - will error if GPU unavailable
  num_workers: 0  # DataLoader workers (0 = main process, faster for small datasets)
  pin_memory: true
  
# Performance Tuning
performance:
  # Gradient accumulation (effective batch size = batch_size * accumulation_steps)
  # Use this to simulate larger batches without increasing memory
  gradient_accumulation_steps: 1
  
  # Mixed precision training (faster on modern GPUs)
  # Set to true to enable automatic mixed precision (AMP)
  mixed_precision: false
  
  # Compile model (PyTorch 2.0+ feature, speeds up training)
  compile_model: false
  
# Inference Configuration
inference:
  max_operations: 100
  beam_width: 5  # Beam search width for operation sequences
  temperature: 1.0  # Sampling temperature
  top_k: 10  # Top-k sampling for operations
