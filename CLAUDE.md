# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Deep learning system for **multi-material 3D printing toolpath optimization**. Minimizes zone crossings (material transitions) in Hamiltonian paths. Data generated by Simulated Annealing (SA) and Genetic Algorithm (GA) optimizers. Targets the CCAI 2026 paper.

Current approach: **Decision Transformer v2** trained on effective-only operations (condensed trajectories). Baseline CNN+RNN kept as reference.

## Common Commands

```bash
# Environment setup (RunPod with RTX 5090)
./setup_runpod.sh
source nn_venv/bin/activate

# Local GPU setup (RTX 4080, CUDA 12.1)
./setup_gpu.sh

# Data generation pipeline (parallel SA runs)
python run_pipeline.py <machine_id> --workers 8
python run_pipeline.py <machine_id> --status
python run_pipeline.py <machine_id> --retry-failed

# Merge datasets from multiple machines
python merge_datasets_safe.py

# Decision Transformer v2 pipeline
python model/decision_transformer/build_dt_training_data.py   # Build effective_dt_data.pkl
python model/decision_transformer/precompute_rtg.py            # Precompute RTG cache
python model/decision_transformer/train_dt_v2.py               # Train DT v2
python model/decision_transformer/inference_dt_v2.py            # Run inference

# Baseline CNN+RNN (reference)
./run_training.sh
python model/train.py --train-file nn_data/train_all.jsonl --val-file nn_data/val_all.jsonl
python model/inference.py --checkpoint nn_checkpoints/best_model.pt --grid-W 30 --grid-H 30 --zone-pattern left_right
```

Always set `PYTHONPATH=$(pwd):$PYTHONPATH` before running model scripts.

## Architecture

### Data Flow

1. **Zone patterns** (`Zones.py`) define material regions on a grid (left_right, top_bottom, diagonal, stripes, checkerboard, voronoi)
2. **SA/GA optimizers** (`SA_generation.py`, `GA_squence.py`) find operation sequences that reduce zone crossings, output JSONL records to `output/`
3. **Pipeline** (`pipeline/`) orchestrates parallel data generation with multiprocessing, Rich UI, checkpointing, and resume
4. **Preprocessing** (`model/data/preprocess.py`) filters (min 40% improvement), stratifies (80/10/10 split by grid size), and encodes data into `nn_data/`

### Core Domain Concepts

**Hamiltonian path**: Path visiting every cell in a grid exactly once, represented by edge matrices H (horizontal) and V (vertical). Initial path patterns: zigzag (default), vertical_zigzag, fermat_spiral, hilbert, snake_bends.

**Operations**: Two path-preserving transforms applied at (x, y) positions:
- **Transpose (T)**: Reroutes a 3x3 subgrid. Variants: nl, nr, sl, sr, eb, wa, wb, ea
- **Flip (F)**: Reverses edges in a 3x2 or 2x3 subgrid. Variants: n, s, e, w

**Zone crossing**: An edge where the path crosses between different material zones. The optimization goal is to minimize these.

**Key SA insight**: Every effective operation reduces crossings by exactly 2. Effective ops cluster at zone boundaries. SA wastes ~97.5% of operations on exploration.

### Decision Transformer v2 (model/decision_transformer/)

Current primary approach. Conditions action generation on return-to-go (RTG).

**Pipeline**: `build_dt_training_data.py` extracts effective-only ops from raw JSONL, condensing trajectories → `precompute_rtg.py` builds RTG cache → `train_dt_v2.py` trains → `inference_dt_v2.py` evaluates.

**Model** (`dt_model.py`): SpatialStateEncoder with AdaptiveAvgPool2d(4), embed_dim=256, n_heads=8, n_layers=6. ~5.4M parameters.

**Dataset** (`dt_dataset_v2.py`): Loads effective_dt_data.pkl, sliding window sampling, trajectory-level train/val split. 12 canonical variant classes (8 transpose + 4 flip).

**Precomputed data files**: `effective_dt_data.pkl` (~3.5GB, effective trajectories with grid states) and `rtg_cache.pkl` (~89MB, return-to-go values). These are generated by the build/precompute scripts and must exist before training.

**Known limitation**: Spatial compression bottleneck — 30x30 grid compressed to 256-dim vector destroys fine-grained position info needed for operation placement. Position accuracy plateaus at ~22% x, ~11% y.

### Baseline CNN+RNN (model/ — reference only)

**CNN backbone** (`models/cnn_rnn.py`): 4 conv layers (16->32->64->128 channels), 3x3 kernels, batch norm, global avg pool -> 128-dim embedding. Input: 4 channels (H_edges, V_edges, zone grid, mask).

**RNN solver**: GRU (256 hidden, 2 layers) processes sequence of CNN embeddings.

**Multi-head predictor**: op type (3-class), position X/Y (30-class each), flip variant (4-class), transpose variant (5-class).

**Loss** (`training/loss.py`): Multi-task cross-entropy with label smoothing (0.1).

### Key Configuration

CNN+RNN hyperparameters in `model/config.yaml`. DT v2 hyperparameters in `train_dt_v2.py` argparse defaults.

### Dataset Format (JSONL)

Each line represents one complete SA/GA optimization run:
```json
{
  "run_id": "sa_left_right_W30H30_seed3_...",
  "grid_W": 30, "grid_H": 30,
  "zone_pattern": "left_right",
  "zone_grid": [0, 0, ..., 1, 1],
  "initial_crossings": 30, "final_crossings": 14,
  "sequence_ops": [{"kind": "T", "x": 0, "y": 24, "variant": "nl"}, ...]
}
```

## Important Paths

| Path | Purpose |
|------|---------|
| `operations.py` | HamiltonianSTL class — grid path manipulation |
| `SA_generation.py` | Simulated Annealing optimizer |
| `GA_squence.py` | Genetic Algorithm with state-aware crossover |
| `Zones.py` | Zone pattern definitions (6 patterns) |
| `pipeline/runner.py` | Multiprocessing orchestrator with Rich UI |
| `model/config.yaml` | CNN+RNN hyperparameters |
| `model/models/cnn_rnn.py` | CNN+RNN architecture (reference) |
| `model/training/loss.py` | Multi-task loss (HamiltonianLoss) |
| `model/data/dataset.py` | HamiltonianDataset (PyTorch Dataset) |
| `model/decision_transformer/dt_model.py` | Decision Transformer architecture |
| `model/decision_transformer/dt_dataset_v2.py` | DT v2 dataset with sliding windows |
| `model/decision_transformer/train_dt_v2.py` | DT v2 training script |
| `model/decision_transformer/inference_dt_v2.py` | DT v2 inference with evaluation |
| `model/decision_transformer/build_dt_training_data.py` | Extract effective ops from JSONL |
| `model/decision_transformer/precompute_rtg.py` | Precompute RTG cache |
| `config/global_config.yaml` | SA algorithm presets (short/medium/long/extra_long) |
| `pipeline/config.py` | Pipeline data classes and YAML config loading |
| `pipeline/checkpoint.py` | Resumable checkpoint management |
| `pipeline/worker.py` | Subprocess-based SA worker execution |
| `pipeline/task_generator.py` | Task generation and filtering |
| `merge_datasets_safe.py` | Merge JSONL datasets from multiple machines |

## Notes

- **No test suite**: There are no unit tests or test directories. Validation is done through training metrics and inference evaluation.
- **All domain logic is self-contained**: Custom implementations for Hamiltonian paths, SA, and GA (no NetworkX, scipy.optimize, or DEAP).
- **Multi-machine data generation**: Pipeline uses `machine_id` for checkpoint isolation; `merge_datasets_safe.py` combines results.
- **GA file is intentionally misspelled** as `GA_squence.py` (not `sequence`).
